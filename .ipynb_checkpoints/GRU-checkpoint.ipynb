{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry = []\n",
    "tf_word = {}\n",
    "with open('poetry.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = re.sub('（\\S+）', '', line)\n",
    "        line = re.sub('_', '', line)\n",
    "        poetry += re.split('[，。！？；]', line.strip())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_words_poetry = list(filter(lambda x: len(x)==5, poetry))\n",
    "seven_words_poetry = list(filter(lambda x: len(x)==7, poetry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒随穷律变', '春逐鸟声开', '初风飘带柳', '晚雪间花梅', '碧林青旧竹']\n",
      "['暧暧去尘昏灞岸', '飞飞轻盖指河梁', '云峰衣结千重叶', '雪岫花开几树妆', '深悲黄鹤孤舟远']\n"
     ]
    }
   ],
   "source": [
    "print(five_words_poetry[:5], seven_words_poetry[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "五言诗诗句总数:296827，七言诗诗句总数:142637\n"
     ]
    }
   ],
   "source": [
    "print('五言诗诗句总数:{}，七言诗诗句总数:{}'.format(len(five_words_poetry), len(seven_words_poetry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒', '随', '穷', '律', '变']\n",
      "['春', '逐', '鸟', '声', '开']\n",
      "['初', '风', '飘', '带', '柳']\n",
      "['晚', '雪', '间', '花', '梅']\n",
      "['碧', '林', '青', '旧', '竹']\n",
      "['绿', '沼', '翠', '新', '苔']\n",
      "['芝', '田', '初', '雁', '去']\n",
      "['绮', '树', '巧', '莺', '来']\n",
      "['晚', '霞', '聊', '自', '怡']\n",
      "['初', '晴', '弥', '可', '喜']\n"
     ]
    }
   ],
   "source": [
    "word_seq = []\n",
    "for line in five_words_poetry:\n",
    "    word_seq.append([word for word in line])\n",
    "print(*word_seq[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, sentences, word_vec_size):\n",
    "        self.sentences = sentences\n",
    "        self.word_vec_size = word_vec_size\n",
    "        \n",
    "        self.model = None\n",
    "        self.embedding_matrix = None\n",
    "        \n",
    "        self._word2idx = {}\n",
    "        \n",
    "    def word2idx(self, word):\n",
    "        if self.model is None:\n",
    "            raise NameError('No model, use mk_embedding first.')\n",
    "        return self._word2idx[word]\n",
    "    \n",
    "    def idx2word(self, i):\n",
    "        if self.model is None:\n",
    "            raise NameError('No model, use mk_embedding first.')\n",
    "        return self.model.wv.index2word[i]\n",
    "    \n",
    "    def mk_embedding(self, load_model_path=None, save_model_path=None):\n",
    "        if load_model_path is not None:\n",
    "            model = Word2Vec.load(load_model_path)\n",
    "        else:\n",
    "            print('Word2Vec training ...')\n",
    "            model = Word2Vec(self.sentences, size=self.word_vec_size, window=5, min_count=20, workers=12, iter=10, sg=1)\n",
    "            if save_model_path is not None:\n",
    "                model.save(save_model_path)\n",
    "            else:\n",
    "                model.save('embedding/{}_word2vec.model'.format(self.word_vec_size))\n",
    "        \n",
    "        self.model = model\n",
    "        self.embedding_matrix = model.wv.vectors\n",
    "        for i, word in enumerate(model.wv.index2word):\n",
    "            self._word2idx[word] = i\n",
    "            \n",
    "        print('Embedding OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding OK.\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding(word_seq, 250)\n",
    "embedding.mk_embedding(load_model_path='embedding/200_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_seq = []\n",
    "for line in word_seq:\n",
    "    try:\n",
    "        idx_seq.append([embedding.word2idx(word) for word in line])\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 163, 282, 1106, 514]\n",
      "[17, 406, 126, 69, 90]\n",
      "[171, 5, 452, 352, 238]\n",
      "[164, 116, 179, 23, 666]\n",
      "[267, 88, 55, 143, 160]\n",
      "[236, 1701, 280, 76, 472]\n",
      "[1166, 325, 171, 299, 36]\n",
      "[829, 73, 1510, 714, 12]\n",
      "[164, 372, 747, 22, 1976]\n",
      "[171, 388, 1120, 82, 475]\n",
      "用于训练的诗句数:289275\n"
     ]
    }
   ],
   "source": [
    "print(*idx_seq[:10], sep='\\n')\n",
    "print('用于训练的诗句数:{}'.format(len(idx_seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, num_words, num_layers=1, dropout=0, fix_embedding=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = True if fix_embedding is True else False\n",
    "\n",
    "        self.rnn = nn.GRU(embedding_matrix.shape[1], hidden_size, num_layers,\n",
    "                          dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Dropout(0.2),\n",
    "                                        nn.Linear(hidden_size, hidden_size*2),\n",
    "                                        nn.ReLU(),\n",
    "                                        \n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(hidden_size*2, hidden_size*4),\n",
    "                                        nn.ReLU(),\n",
    "                                        \n",
    "                                        nn.Linear(hidden_size*4, num_words),\n",
    "                                        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input shape is (batch_size, sequence_len, word_size)\n",
    "        # hidden shape is (batch_size, num_directions*num_layers, hidden_size)\n",
    "        embedding_seq = self.embedding(input[:,:-1])\n",
    "        output, _ = self.rnn(embedding_seq, None)\n",
    "        pred = self.classifier(output)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "def x2y(x):\n",
    "    y = [line[1:] for line in x]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(x2y(idx_seq))\n",
    "x = np.array(idx_seq)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "train_set = DataLoader(PoetryDataset(x_train, y_train), batch_size=128)\n",
    "val_set = DataLoader(PoetryDataset(x_val, y_val), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 20\n",
    "hidden_size = 250\n",
    "num_words = len(embedding.model.wv.index2word)\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters:6681238\n"
     ]
    }
   ],
   "source": [
    "model = RNN(embedding.embedding_matrix, hidden_size, num_words, num_layers=3)\n",
    "model = model.to(device=device)\n",
    "print('total parameters:{}'.format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "calc_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(pred, y):\n",
    "    correct = np.sum(np.argmax(pred.detach().cpu().numpy(), axis=1) == y.detach().cpu().numpy())\n",
    "    return correct / (pred.shape[0] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[01/10] time:17.65(sec) train_loss:6.95082 train_acc:0.01711 | val_loss:6.63738 val_acc:0.02775\n",
      "epoch:[02/10] time:17.42(sec) train_loss:6.42365 train_acc:0.03881 | val_loss:6.24692 val_acc:0.05140\n",
      "epoch:[03/10] time:17.50(sec) train_loss:6.09725 train_acc:0.05913 | val_loss:6.02653 val_acc:0.06876\n",
      "epoch:[04/10] time:17.43(sec) train_loss:5.87440 train_acc:0.07323 | val_loss:5.90063 val_acc:0.07914\n",
      "epoch:[05/10] time:17.33(sec) train_loss:5.70832 train_acc:0.08283 | val_loss:5.82843 val_acc:0.08552\n",
      "epoch:[06/10] time:17.41(sec) train_loss:5.57362 train_acc:0.09002 | val_loss:5.78889 val_acc:0.09015\n",
      "epoch:[07/10] time:17.34(sec) train_loss:5.46001 train_acc:0.09533 | val_loss:5.77474 val_acc:0.09329\n",
      "epoch:[08/10] time:17.35(sec) train_loss:5.36131 train_acc:0.10002 | val_loss:5.77734 val_acc:0.09514\n",
      "epoch:[09/10] time:17.44(sec) train_loss:5.27545 train_acc:0.10362 | val_loss:5.79448 val_acc:0.09682\n",
      "epoch:[10/10] time:17.39(sec) train_loss:5.19859 train_acc:0.10703 | val_loss:5.82050 val_acc:0.09803\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_set:\n",
    "        x = data[0].to(dtype=torch.long, device=device)\n",
    "        y = data[1].to(dtype=torch.long, device=device)\n",
    "        # y shape is (batch, seq)\n",
    "    \n",
    "        pred = model(x).transpose(1, 2)\n",
    "        # pred shape should be (batch, categories_prob, seq)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = calc_loss(pred, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += batch_loss / len(train_set)\n",
    "        train_acc += calc_acc(pred, y) / len(train_set)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_set:\n",
    "            x = data[0].to(dtype=torch.long, device=device)\n",
    "            y = data[1].to(dtype=torch.long, device=device)\n",
    "            \n",
    "            pred = model(x).transpose(1, 2)\n",
    "            batch_loss = calc_loss(pred, y)\n",
    "            \n",
    "            val_loss += batch_loss / len(val_set)\n",
    "            val_acc += calc_acc(pred, y) / len(val_set)\n",
    "            \n",
    "    print('epoch:[{:02d}/{:02d}] time:{:2.2f}(sec) train_loss:{:2.5f} train_acc:{:2.5f} | val_loss:{:2.5f} val_acc:{:2.5f}'.format(epoch+1, num_epoch, time.time() - start_time, train_loss, train_acc, val_loss, val_acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model, data, embedding):\n",
    "        self.model = model \n",
    "        self.data = data\n",
    "        self.embedding = embedding\n",
    "    \n",
    "    def generate(self, diff, num_sample):\n",
    "        \"\"\"\n",
    "        diff: the difference generated verse between original verse\n",
    "        \"\"\"\n",
    "        i = random.sample(range(self.data.shape[0]), num_sample)\n",
    "        x = torch.tensor(self.data[i,:], dtype=torch.long, device=device)\n",
    "        original = [list(map(self.embedding.idx2word, line)) for line in x]\n",
    "        first_words = [list(map(self.embedding.idx2word, line))[0] for line in x.detach().cpu().numpy()]\n",
    "        pred = model(x)\n",
    "        pred = np.argsort(pred.detach().cpu().numpy(), axis=2)[:,:,::-1]\n",
    "        pred = [pred[:,:,diff] for _ in range(num_sample)][0]\n",
    "        return original, [[first_word] + list(map(self.embedding.idx2word, line)) for first_word, line in zip(first_words, pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(model, x_val, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原诗:\t\t\t\t生成诗：\n",
      "['穷', '达', '恋', '明', '主']\t['穷', '通', '不', '何', '主']\n",
      "['南', '征', '又', '北', '移']\t['南', '山', '何', '何', '归']\n",
      "['重', '上', '北', '城', '看']\t['重', '阳', '望', '邙', '楼']\n",
      "['禺', '山', '金', '碧', '路']\t['禺', '山', '入', '谷', '色']\n",
      "['登', '台', '古', '寺', '中']\t['登', '楼', '望', '木', '深']\n",
      "['苟', '无', '济', '代', '心']\t['苟', '能', '天', '世', '才']\n",
      "['笳', '繁', '思', '落', '梅']\t['笳', '箫', '叶', '不', '日']\n",
      "['裴', '回', '吴', '越', '间']\t['裴', '回', '望', '岫', '间']\n",
      "['农', '谈', '四', '邻', '夕']\t['农', '夫', '不', '海', '里']\n",
      "['天', '晴', '虹', '影', '渡']\t['天', '涯', '见', '气', '起']\n"
     ]
    }
   ],
   "source": [
    "original, generate = generator.generate(0, 10)\n",
    "print(\"原诗:\\t\\t\\t\\t生成诗：\")\n",
    "for i,j in zip(original, generate):\n",
    "    print(\"{}\\t{}\".format(i, j))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
