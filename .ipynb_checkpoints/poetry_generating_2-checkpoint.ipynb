{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry = []\n",
    "tf_word = {}\n",
    "with open('poetry.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = re.sub('（\\S+）', '', line)\n",
    "        for word in line:\n",
    "            if word not in tf_word:\n",
    "                tf_word[word] = 1\n",
    "            else:\n",
    "                tf_word[word] += 1\n",
    "        if len(line) > 15:\n",
    "            poetry += re.split('[，。！？；]', line.strip())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_words_poetry = list(filter(lambda x: len(x) == 5, poetry))\n",
    "seven_words_poetry = list(filter(lambda x: len(x) == 7, poetry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒随穷律变', '春逐鸟声开', '初风飘带柳', '晚雪间花梅', '碧林青旧竹']\n",
      "['暧暧去尘昏灞岸', '飞飞轻盖指河梁', '云峰衣结千重叶', '雪岫花开几树妆', '深悲黄鹤孤舟远']\n"
     ]
    }
   ],
   "source": [
    "print(five_words_poetry[:5], seven_words_poetry[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "五言诗诗句总数:296255，七言诗诗句总数:141968\n"
     ]
    }
   ],
   "source": [
    "print('五言诗诗句总数:{}，七言诗诗句总数:{}'.format(len(five_words_poetry), len(seven_words_poetry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tf_word = 150\n",
    "# 过滤出现次数小于该值的字\n",
    "word_seq = []\n",
    "word2idx = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in five_words_poetry:\n",
    "    words = [word if tf_word[word] > min_tf_word else '<UNK>' for word in line]\n",
    "    word_seq.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['寒', '随', '穷', '律', '变'], ['春', '逐', '鸟', '声', '开'], ['初', '风', '飘', '带', '柳'], ['晚', '雪', '间', '花', '梅'], ['碧', '林', '青', '旧', '竹'], ['绿', '沼', '翠', '新', '苔'], ['芝', '田', '初', '雁', '去'], ['绮', '树', '巧', '莺', '来'], ['晚', '霞', '聊', '自', '<UNK>'], ['初', '晴', '弥', '可', '喜'], ['日', '<UNK>', '百', '花', '色'], ['风', '动', '千', '林', '翠'], ['池', '鱼', '跃', '不', '同'], ['园', '鸟', '声', '还', '异'], ['寄', '言', '博', '通', '者'], ['知', '予', '物', '外', '志'], ['一', '朝', '春', '夏', '改'], ['隔', '夜', '鸟', '花', '迁'], ['阴', '阳', '深', '浅', '叶'], ['晓', '夕', '重', '轻', '烟']]\n"
     ]
    }
   ],
   "source": [
    "print(word_seq[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "    \n",
    "    def mk_embedding(self):\n",
    "        for line in self.data:\n",
    "            for word in line:\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = len(self.idx2word)\n",
    "                    self.idx2word.append(word)\n",
    "                    \n",
    "    def word_seq2idx_seq(self, word_seq):\n",
    "        idx_seq = []\n",
    "        for line in word_seq:\n",
    "            idx_seq.append(list(map(lambda word: embedding.word2idx[word], line)))\n",
    "        return idx_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(word_seq)\n",
    "embedding.mk_embedding()\n",
    "idx_seq = embedding.word_seq2idx_seq(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n"
     ]
    }
   ],
   "source": [
    "print(idx_seq[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i,:], self.y[i,:]\n",
    "\n",
    "def x2y(x):\n",
    "    return x[1:]\n",
    "    \n",
    "x_train = np.array(idx_seq)\n",
    "y_train = np.array(list(map(lambda x: (x2y(x)), idx_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[   0    1    2    3    4]\n",
      " [   5    6    7    8    9]\n",
      " [  10   11   12   13   14]\n",
      " ...\n",
      " [ 170   27   42  149  264]\n",
      " [  42   42  734 1731   42]\n",
      " [ 172  994 1530  451 2055]]\n",
      "y:\n",
      "[[   1    2    3    4]\n",
      " [   6    7    8    9]\n",
      " [  11   12   13   14]\n",
      " ...\n",
      " [  27   42  149  264]\n",
      " [  42  734 1731   42]\n",
      " [ 994 1530  451 2055]]\n"
     ]
    }
   ],
   "source": [
    "print('x:\\n{}\\ny:\\n{}'.format(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(PoetryDataset(x_train, y_train), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(*embedding_size)\n",
    "        self.embedding.weight = torch.nn.Parameter(torch.randn(*embedding_size))\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(embedding_size[1], hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
    "                                              torch.nn.Linear(self.hidden_size, self.hidden_size//2),\n",
    "                                              torch.nn.Sigmoid(),\n",
    "                                              \n",
    "                                              torch.nn.Dropout(0.2),\n",
    "                                              torch.nn.Linear(self.hidden_size//2, self.hidden_size//4),\n",
    "                                              torch.nn.Sigmoid(),\n",
    "                                              \n",
    "                                              torch.nn.Dropout(0.2),\n",
    "                                              torch.nn.Linear(self.hidden_size//4, self.embedding_size[0]),\n",
    "                                             )\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        hidden, _ = self.lstm(x.to(dtype=torch.float32), None)\n",
    "        y = self.classifier(hidden)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(embedding.idx2word)\n",
    "# 诗句中出现的词总数\n",
    "num_word_size = 200\n",
    "# embedding后每个单词的维度\n",
    "num_hidden_size = 1024\n",
    "# LSTM隐藏层输出维度\n",
    "num_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN((num_words, num_word_size), num_word_size)\n",
    "model.to(device='cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "calc_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(pred, y):\n",
    "    pred = np.argmax(pred.detach().cpu().numpy(), axis=1)\n",
    "    return (y.detach().cpu().numpy() == pred).sum() / pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/005] time:23.91(sec) loss:7.2610 acc:0.0434\n",
      "[002/005] time:23.97(sec) loss:7.2500 acc:0.0763\n",
      "[003/005] time:23.23(sec) loss:7.2436 acc:0.0595\n",
      "[004/005] time:22.14(sec) loss:7.2180 acc:0.0504\n",
      "[005/005] time:23.92(sec) loss:7.1965 acc:0.0838\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = torch.load('5_epoch_model.pkl')\n",
    "except FileNotFoundError:\n",
    "    model.train()\n",
    "    for epoch in range(num_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        for data in train_set:\n",
    "            x = data[0].to(device='cuda', dtype=torch.long)\n",
    "            y = data[1].to(device='cuda', dtype=torch.long)\n",
    "            pred = model(x)[:,:-1,:]\n",
    "            optimizer.zero_grad()\n",
    "            # 去掉预测序列的最后最后一个值\n",
    "            pred = pred.transpose(1,2)\n",
    "            batch_loss = calc_loss(pred, y)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += batch_loss / len(train_set)\n",
    "            train_acc += calc_acc(pred, y) / len(train_set)\n",
    "\n",
    "        print('[{:03d}/{:03d}] time:{:.2f}(sec) loss:{:.4f} acc:{:.4f}'.format(epoch+1, num_epoch, time.time()-start_time, train_loss, train_acc))\n",
    "\n",
    "    torch.save(model, '{}_epoch_model.pkl'.format(num_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, data, model, embedding):\n",
    "        self.data = data\n",
    "        self.model = model.eval()\n",
    "        self.embedding = embedding\n",
    "    \n",
    "    def idx2word(self, idx_seq):\n",
    "        return list(map(lambda x:self.embedding.idx2word[x], idx_seq[0]))\n",
    "        \n",
    "    def generate(self):\n",
    "        i = random.randint(0, len(self.data))\n",
    "        x = torch.tensor(self.data[i], device=torch.device('cuda'), dtype=torch.long).unsqueeze(0)\n",
    "        pred = self.model(x)\n",
    "        prob_pred = np.argsort(pred.detach().cpu().numpy(), axis=2)\n",
    "        idx_pred = np.zeros((1,5))\n",
    "        for i in range(5):\n",
    "            rand_word_idx = np.random.randint(0,5,1)\n",
    "            idx_pred[0,i] = prob_pred[:,i,rand_word_idx]\n",
    "        idx_pred = idx_pred.astype(np.long)\n",
    "        return [self.idx2word(x)[0]] + self.idx2word(idx_pred)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(x_train, model, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['鞭', '笺', '娟', '寞', '倏']\n",
      "['阴', '每', '寞', '樱', '但']\n",
      "['残', '笺', '娟', '牡', '但']\n",
      "['愁', '娟', '寞', '娟', '只']\n",
      "['意', '蓉', '蓉', '辱', '偷']\n",
      "['秋', '偷', '蓉', '樱', '偷']\n",
      "['林', '霖', '翩', '扁', '笺']\n",
      "['眼', '跎', '娟', '牡', '娟']\n",
      "['余', '蓉', '寞', '樱', '但']\n",
      "['长', '寞', '娟', '辱', '递']\n",
      "['春', '偷', '寞', '或', '偷']\n",
      "['孤', '聊', '寞', '特', '但']\n",
      "['精', '寞', '娟', '特', '递']\n",
      "['誓', '蓉', '翩', '樱', '聊']\n",
      "['弟', '蓉', '跎', '翩', '寞']\n",
      "['秋', '倏', '寞', '跎', '蓉']\n",
      "['独', '寞', '翁', '笺', '倏']\n",
      "['宿', '讵', '跎', '娟', '偷']\n",
      "['又', '跎', '蓉', '笺', '递']\n",
      "['愿', '跎', '寞', '跎', '跎']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(generator.generate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
