{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry = []\n",
    "tf_word = {}\n",
    "with open('poetry.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = re.sub('（\\S+）', '', line)\n",
    "        line = re.sub('_', '', line)\n",
    "        poetry += re.split('[，。！？；]', line.strip())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_words_poetry = list(filter(lambda x: len(x)==5, poetry))\n",
    "seven_words_poetry = list(filter(lambda x: len(x)==7, poetry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒随穷律变', '春逐鸟声开', '初风飘带柳', '晚雪间花梅', '碧林青旧竹']\n",
      "['暧暧去尘昏灞岸', '飞飞轻盖指河梁', '云峰衣结千重叶', '雪岫花开几树妆', '深悲黄鹤孤舟远']\n"
     ]
    }
   ],
   "source": [
    "print(five_words_poetry[:5], seven_words_poetry[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "五言诗诗句总数:296827，七言诗诗句总数:142637\n"
     ]
    }
   ],
   "source": [
    "print('五言诗诗句总数:{}，七言诗诗句总数:{}'.format(len(five_words_poetry), len(seven_words_poetry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒', '随', '穷', '律', '变']\n",
      "['春', '逐', '鸟', '声', '开']\n",
      "['初', '风', '飘', '带', '柳']\n",
      "['晚', '雪', '间', '花', '梅']\n",
      "['碧', '林', '青', '旧', '竹']\n",
      "['绿', '沼', '翠', '新', '苔']\n",
      "['芝', '田', '初', '雁', '去']\n",
      "['绮', '树', '巧', '莺', '来']\n",
      "['晚', '霞', '聊', '自', '怡']\n",
      "['初', '晴', '弥', '可', '喜']\n"
     ]
    }
   ],
   "source": [
    "word_seq = []\n",
    "for line in five_words_poetry:\n",
    "    word_seq.append([word for word in line])\n",
    "print(*word_seq[:10], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, sentences, word_vec_size):\n",
    "        self.sentences = sentences\n",
    "        self.word_vec_size = word_vec_size\n",
    "        \n",
    "        self.model = None\n",
    "        self.embedding_matrix = None\n",
    "        \n",
    "        self._word2idx = {}\n",
    "        \n",
    "    def word2idx(self, word):\n",
    "        if self.model is None:\n",
    "            raise NameError('No model, use mk_embedding first.')\n",
    "        return self._word2idx[word]\n",
    "    \n",
    "    def idx2word(self, i):\n",
    "        if self.model is None:\n",
    "            raise NameError('No model, use mk_embedding first.')\n",
    "        return self.model.wv.index2word[i]\n",
    "    \n",
    "    def mk_embedding(self, load_model_path=None, save_model_path=None):\n",
    "        if load_model_path is not None:\n",
    "            model = Word2Vec.load(load_model_path)\n",
    "        else:\n",
    "            print('Word2Vec training ...')\n",
    "            model = Word2Vec(self.sentences, size=self.word_vec_size, window=5, min_count=20, workers=12, iter=10, sg=1)\n",
    "            if save_model_path is not None:\n",
    "                model.save(save_model_path)\n",
    "            else:\n",
    "                model.save('embedding/{}_word2vec.model'.format(self.word_vec_size))\n",
    "        \n",
    "        self.model = model\n",
    "        self.embedding_matrix = model.wv.vectors\n",
    "        for i, word in enumerate(model.wv.index2word):\n",
    "            self._word2idx[word] = i\n",
    "            \n",
    "        print('Embedding OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding OK.\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding(word_seq, 250)\n",
    "embedding.mk_embedding(load_model_path='embedding/200_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_seq = []\n",
    "for line in word_seq:\n",
    "    try:\n",
    "        idx_seq.append([embedding.word2idx(word) for word in line])\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 163, 282, 1106, 514]\n",
      "[17, 406, 126, 69, 90]\n",
      "[171, 5, 452, 352, 238]\n",
      "[164, 116, 179, 23, 666]\n",
      "[267, 88, 55, 143, 160]\n",
      "[236, 1701, 280, 76, 472]\n",
      "[1166, 325, 171, 299, 36]\n",
      "[829, 73, 1510, 714, 12]\n",
      "[164, 372, 747, 22, 1976]\n",
      "[171, 388, 1120, 82, 475]\n",
      "用于训练的诗句数:289275\n"
     ]
    }
   ],
   "source": [
    "print(*idx_seq[:10], sep='\\n')\n",
    "print('用于训练的诗句数:{}'.format(len(idx_seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, num_layers=1, dropout=0, fix_embedding=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = True if fix_embedding is True else False\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_matrix.shape[1], hidden_size, num_layers,\n",
    "                          dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input.shape is (batch size, sequence len, vocab size)\n",
    "        embedding_seq = self.embedding(input)\n",
    "        output, hidden = self.rnn(self.dropout(embedding_seq))\n",
    "        # outputs shape is (batch_size, sequence_len, hid_size * directions)\n",
    "        # hidden shape is (num_layers * directions, batch_size, hid_size)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, encoder_output, decoder_hidden):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, num_words, num_layers=1, dropout=0, fix_embedding=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = True if fix_embedding is True else False\n",
    "\n",
    "        self.rnn = nn.GRU(embedding_matrix.shape[1], hidden_size, num_layers,\n",
    "                          dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Dropout(0.2),\n",
    "                                        nn.Linear(hidden_size, hidden_size*2),\n",
    "                                        nn.ReLU(),\n",
    "                                        \n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(hidden_size*2, hidden_size*4),\n",
    "                                        nn.ReLU(),\n",
    "                                        \n",
    "                                        nn.Linear(hidden_size*4, num_words),\n",
    "                                        )\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input shape is (batch_size, sequence_len, word_size)\n",
    "        # hidden shape is (batch_size, num_directions*num_layers, hidden_size)\n",
    "        embedding_seq = self.embedding(input[:,:-1])\n",
    "        output, _ = self.rnn(embedding_seq, hidden)\n",
    "        pred = self.classifier(output)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(device=device)\n",
    "        self.decoder = decoder.to(device=device)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output, _ = self.encoder(input)\n",
    "        # outputs shape is (batch_size, sequence_len, hid_size * directions)\n",
    "        hidden = output[:,-1,:].unsqueeze(0)\n",
    "        pred = self.decoder(input, hidden)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "def x2y(x):\n",
    "    y = [line[1:] for line in x]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(x2y(idx_seq))\n",
    "x = np.array(idx_seq)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "train_set = DataLoader(PoetryDataset(x_train, y_train), batch_size=128)\n",
    "val_set = DataLoader(PoetryDataset(x_val, y_val), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 5\n",
    "hidden_size = 250\n",
    "num_words = len(embedding.model.wv.index2word)\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters:14983238\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(embedding.embedding_matrix, hidden_size)\n",
    "decoder = Decoder(embedding.embedding_matrix, hidden_size*2, num_words)\n",
    "# hidden_size * 2 for the bidirection\n",
    "model = Seq2Seq(encoder, decoder, device)\n",
    "print('total parameters:{}'.format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "calc_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(pred, y):\n",
    "    correct = np.sum(np.argmax(pred.detach().cpu().numpy(), axis=1) == y.detach().cpu().numpy())\n",
    "    return correct / (pred.shape[0] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[01/05] time:24.42(sec) train_loss:4.61956 train_acc:0.17496 | val_loss:2.21130 val_acc:0.48622\n",
      "epoch:[02/05] time:24.13(sec) train_loss:1.49382 train_acc:0.61506 | val_loss:0.59737 val_acc:0.83870\n",
      "epoch:[03/05] time:24.28(sec) train_loss:0.52512 train_acc:0.84634 | val_loss:0.22049 val_acc:0.93754\n",
      "epoch:[04/05] time:24.40(sec) train_loss:0.23467 train_acc:0.92739 | val_loss:0.09952 val_acc:0.97106\n",
      "epoch:[05/05] time:25.39(sec) train_loss:0.13252 train_acc:0.95823 | val_loss:0.05811 val_acc:0.98294\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_set:\n",
    "        x = data[0].to(dtype=torch.long, device=torch.device('cuda:1'))\n",
    "        y = data[1].to(dtype=torch.long, device=torch.device('cuda:1'))\n",
    "        # y shape is (batch, seq)\n",
    "        \n",
    "        pred = model(x).transpose(1, 2)\n",
    "        # pred shape should be (batch, categories_prob, seq)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = calc_loss(pred, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += batch_loss / len(train_set)\n",
    "        train_acc += calc_acc(pred, y) / len(train_set)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_set:\n",
    "            x = data[0].to(dtype=torch.long, device=torch.device('cuda:1'))\n",
    "            y = data[1].to(dtype=torch.long, device=torch.device('cuda:1'))\n",
    "            \n",
    "            pred = model(x).transpose(1, 2)\n",
    "            batch_loss = calc_loss(pred, y)\n",
    "            \n",
    "            val_loss += batch_loss / len(val_set)\n",
    "            val_acc += calc_acc(pred, y) / len(val_set)\n",
    "            \n",
    "    print('epoch:[{:02d}/{:02d}] time:{:2.2f}(sec) train_loss:{:2.5f} train_acc:{:2.5f} | val_loss:{:2.5f} val_acc:{:2.5f}'.format(epoch+1, num_epoch, time.time() - start_time, train_loss, train_acc, val_loss, val_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'encoder_decoder_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model, data, embedding):\n",
    "        self.model = model \n",
    "        self.data = data\n",
    "        self.embedding = embedding\n",
    "    \n",
    "    def generate(self, diff, num_sample):\n",
    "        \"\"\"\n",
    "        diff: the difference generated verse between original verse\n",
    "        \"\"\"\n",
    "        i = random.sample(range(self.data.shape[0]), num_sample)\n",
    "        x = torch.tensor(self.data[i,:], dtype=torch.long, device=device)\n",
    "        original = [list(map(self.embedding.idx2word, line)) for line in x]\n",
    "        first_words = [list(map(self.embedding.idx2word, line))[0] for line in x.detach().cpu().numpy()]\n",
    "        pred = model(x)\n",
    "        pred = np.argsort(pred.detach().cpu().numpy(), axis=2)[:,:,::-1]\n",
    "        pred = [pred[:,:,diff] for _ in range(num_sample)][0]\n",
    "        return original, [[first_word] + list(map(self.embedding.idx2word, line)) for first_word, line in zip(first_words, pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(model, x_val, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原诗:\t\t\t\t生成诗：\n",
      "['迢', '迢', '入', '紫', '烟']\t['迢', '望', '向', '醮', '阴']\n",
      "['西', '峰', '有', '禅', '老']\t['西', '扃', '与', '在', '好']\n",
      "['王', '祥', '得', '佩', '刀']\t['王', '精', '识', '裾', '局']\n",
      "['陇', '树', '久', '苍', '然']\t['陇', '雨', '异', '氛', '绝']\n",
      "['庶', '往', '共', '饥', '渴']\t['庶', '谪', '更', '煎', '短']\n",
      "['春', '夜', '特', '来', '游']\t['春', '向', '为', '归', '望']\n",
      "['涧', '冰', '妨', '鹿', '饮']\t['涧', '筠', '妨', '步', '讶']\n",
      "['八', '月', '秋', '堂', '琴']\t['八', '秋', '色', '箫', '舷']\n",
      "['萧', '条', '孤', '兴', '发']\t['萧', '兰', '波', '暂', '早']\n",
      "['不', '枉', '故', '人', '驾']\t['不', '暇', '路', '逢', '陟']\n"
     ]
    }
   ],
   "source": [
    "original, generate = generator.generate(1, 10)\n",
    "print(\"原诗:\\t\\t\\t\\t生成诗：\")\n",
    "for i,j in zip(original, generate):\n",
    "    print(\"{}\\t{}\".format(i, j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 还伤迟暮年 还悲又晚时"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
