{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry = []\n",
    "tf_word = {}\n",
    "with open('poetry.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = re.sub('（\\S+）', '', line)\n",
    "        line = re.sub('_', '', line)\n",
    "        poetry += re.split('[，。！？；]', line.strip())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_words_poetry = list(filter(lambda x: len(x)==5, poetry))\n",
    "seven_words_poetry = list(filter(lambda x: len(x)==7, poetry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒随穷律变', '春逐鸟声开', '初风飘带柳', '晚雪间花梅', '碧林青旧竹']\n",
      "['暧暧去尘昏灞岸', '飞飞轻盖指河梁', '云峰衣结千重叶', '雪岫花开几树妆', '深悲黄鹤孤舟远']\n"
     ]
    }
   ],
   "source": [
    "print(five_words_poetry[:5], seven_words_poetry[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "五言诗诗句总数:296827，七言诗诗句总数:142637\n"
     ]
    }
   ],
   "source": [
    "print('五言诗诗句总数:{}，七言诗诗句总数:{}'.format(len(five_words_poetry), len(seven_words_poetry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒', '随', '穷', '律', '变']\n",
      "['春', '逐', '鸟', '声', '开']\n",
      "['初', '风', '飘', '带', '柳']\n",
      "['晚', '雪', '间', '花', '梅']\n",
      "['碧', '林', '青', '旧', '竹']\n",
      "['绿', '沼', '翠', '新', '苔']\n",
      "['芝', '田', '初', '雁', '去']\n",
      "['绮', '树', '巧', '莺', '来']\n",
      "['晚', '霞', '聊', '自', '怡']\n",
      "['初', '晴', '弥', '可', '喜']\n"
     ]
    }
   ],
   "source": [
    "word_seq = []\n",
    "for line in five_words_poetry:\n",
    "    word_seq.append([word for word in line])\n",
    "print(*word_seq[:10], sep='\\n')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, sentences, word_vec_size):\n",
    "        self.sentences = sentences\n",
    "        self.word_vec_size = word_vec_size\n",
    "        \n",
    "        self.model = None\n",
    "        self.embedding_matrix = None\n",
    "        \n",
    "        self._word2idx = {}\n",
    "        \n",
    "    def word2idx(self, word):\n",
    "        if self.model is None:\n",
    "            raise NameError('No model, use mk_embedding first.')\n",
    "        return self._word2idx[word]\n",
    "    \n",
    "    def idx2word(self, i):\n",
    "        if self.model is None:\n",
    "            raise NameError('No model, use mk_embedding first.')\n",
    "        return self.model.wv.index2word[i]\n",
    "    \n",
    "    def mk_embedding(self, load_model_path=None, save_model_path=None):\n",
    "        if load_model_path is not None:\n",
    "            model = Word2Vec.load(load_model_path)\n",
    "        else:\n",
    "            print('Word2Vec training ...')\n",
    "            model = Word2Vec(self.sentences, size=self.word_vec_size, window=5, min_count=20, workers=12, iter=10, sg=1)\n",
    "            if save_model_path is not None:\n",
    "                model.save(save_model_path)\n",
    "            else:\n",
    "                model.save('embedding/{}_word2vec.model'.format(self.word_vec_size))\n",
    "        \n",
    "        self.model = model\n",
    "        self.embedding_matrix = model.wv.vectors\n",
    "        for i, word in enumerate(model.wv.index2word):\n",
    "            self._word2idx[word] = i\n",
    "            \n",
    "        print('Embedding OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding OK.\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding(word_seq, 250)\n",
    "embedding.mk_embedding(load_model_path='embedding/200_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_seq = []\n",
    "for line in word_seq:\n",
    "    try:\n",
    "        idx_seq.append([embedding.word2idx(word) for word in line])\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47, 163, 282, 1106, 514]\n",
      "[17, 406, 126, 69, 90]\n",
      "[171, 5, 452, 352, 238]\n",
      "[164, 116, 179, 23, 666]\n",
      "[267, 88, 55, 143, 160]\n",
      "[236, 1701, 280, 76, 472]\n",
      "[1166, 325, 171, 299, 36]\n",
      "[829, 73, 1510, 714, 12]\n",
      "[164, 372, 747, 22, 1976]\n",
      "[171, 388, 1120, 82, 475]\n",
      "用于训练的诗句数:289275\n"
     ]
    }
   ],
   "source": [
    "print(*idx_seq[:10], sep='\\n')\n",
    "print('用于训练的诗句数:{}'.format(len(idx_seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, num_layers=1, dropout=0, fix_embedding=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix))\n",
    "        self.embedding.weight.requires_grad = True if fix_embedding is True else False\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_matrix.shape[1], hidden_size, num_layers,\n",
    "                          dropout=dropout, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # input.shape is (batch size, sequence len, vocab size)\n",
    "        embedding_seq = self.embedding(input)\n",
    "        output, hidden = self.rnn(self.dropout(embedding_seq))\n",
    "        # outputs shape is (batch_size, sequence_len, hid_size * directions)\n",
    "        # hidden shape is (num_layers * directions, batch_size, hid_size)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, encoder_output, decoder_hidden):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_words, num_layers=1, dropout=0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.classifier = nn.Sequential(nn.Dropout(0.2),\n",
    "                                        nn.Linear(hidden_size, hidden_size*2),\n",
    "                                        nn.ReLU(),\n",
    "                                        \n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(hidden_size*2, hidden_size*4),\n",
    "                                        nn.ReLU(),\n",
    "                                        \n",
    "                                        nn.Linear(hidden_size*4, num_words),\n",
    "                                        )\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # input shape is (batch_size, sequence_len, word_size)\n",
    "        # hidden shape is (batch_size, num_directions*num_layers, hidden_size)\n",
    "        output, _ = self.rnn(input, hidden)\n",
    "        pred = self.classifier(output)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.to(device=device)\n",
    "        self.decoder = decoder.to(device=device)\n",
    "        # outputs shape is (batch_size, sequence_len, hid_size * directions)\n",
    "        # hidden shape is (num_layers * directions, batch_size, hid_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output, _ = self.encoder(input)\n",
    "        output, hidden = output[:,:-1,:], output[:,-1,:].unsqueeze(0)\n",
    "        pred = self.decoder(output, hidden)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "def x2y(x):\n",
    "    y = [line[1:] for line in x]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(x2y(idx_seq))\n",
    "x = np.array(idx_seq)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "train_set = DataLoader(PoetryDataset(x_train, y_train), batch_size=128)\n",
    "val_set = DataLoader(PoetryDataset(x_val, y_val), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epoch = 5\n",
    "hidden_size = 250\n",
    "num_words = len(embedding.model.wv.index2word)\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters:14298738\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(embedding.embedding_matrix, hidden_size)\n",
    "decoder = Decoder(hidden_size*2, hidden_size*2, num_words)\n",
    "# hidden_size * 2 for the bidirection\n",
    "model = Seq2Seq(encoder, decoder, device)\n",
    "print('total parameters:{}'.format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-4)\n",
    "calc_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(pred, y):\n",
    "    correct = np.sum(np.argmax(pred.detach().cpu().numpy(), axis=1) == y.detach().cpu().numpy())\n",
    "    return correct / (pred.shape[0] * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[01/05] time:24.71(sec) train_loss:2.89026 train_acc:0.44457 | val_loss:0.49929 val_acc:0.88243\n",
      "epoch:[02/05] time:24.40(sec) train_loss:0.32943 train_acc:0.91135 | val_loss:0.10409 val_acc:0.97167\n",
      "epoch:[03/05] time:24.45(sec) train_loss:0.11658 train_acc:0.96569 | val_loss:0.04856 val_acc:0.98583\n",
      "epoch:[04/05] time:24.52(sec) train_loss:0.07313 train_acc:0.97799 | val_loss:0.02982 val_acc:0.99135\n",
      "epoch:[05/05] time:24.44(sec) train_loss:0.05180 train_acc:0.98415 | val_loss:0.02033 val_acc:0.99393\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoch):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for data in train_set:\n",
    "        x = data[0].to(dtype=torch.long, device=torch.device('cuda:1'))\n",
    "        y = data[1].to(dtype=torch.long, device=torch.device('cuda:1'))\n",
    "        # y shape is (batch, seq)\n",
    "        \n",
    "        pred = model(x).transpose(1, 2)\n",
    "        # pred shape should be (batch, categories_prob, seq)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = calc_loss(pred, y)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += batch_loss / len(train_set)\n",
    "        train_acc += calc_acc(pred, y) / len(train_set)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in val_set:\n",
    "            x = data[0].to(dtype=torch.long, device=torch.device('cuda:1'))\n",
    "            y = data[1].to(dtype=torch.long, device=torch.device('cuda:1'))\n",
    "            \n",
    "            pred = model(x).transpose(1, 2)\n",
    "            batch_loss = calc_loss(pred, y)\n",
    "            \n",
    "            val_loss += batch_loss / len(val_set)\n",
    "            val_acc += calc_acc(pred, y) / len(val_set)\n",
    "            \n",
    "    print('epoch:[{:02d}/{:02d}] time:{:2.2f}(sec) train_loss:{:2.5f} train_acc:{:2.5f} | val_loss:{:2.5f} val_acc:{:2.5f}'.format(epoch+1, num_epoch, time.time() - start_time, train_loss, train_acc, val_loss, val_acc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model, data, embedding):\n",
    "        self.model = model \n",
    "        self.data = data\n",
    "        self.embedding = embedding\n",
    "    \n",
    "    def generate(self, num_candidate, num_sample):\n",
    "        i = random.sample(range(self.data.shape[0]), num_sample)\n",
    "        x = torch.tensor(self.data[i,:], dtype=torch.long, device=device)\n",
    "        first_words = [list(map(self.embedding.idx2word, line))[0] for line in x.detach().cpu().numpy()]\n",
    "        pred = model(x)\n",
    "        pred = np.argsort(pred.detach().cpu().numpy(), axis=2)[:,:,-num_candidate:]\n",
    "        # select candidates\n",
    "        pred = [pred[:,:,random.randint(0, num_candidate-1)] for _ in range(num_sample)][0]\n",
    "        \n",
    "        return [[first_word] + list(map(self.embedding.idx2word, line)) for first_word, line in zip(first_words, pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(model, x_val, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['相', '看', '话', '离', '合'],\n",
       " ['讵', '言', '才', '不', '才'],\n",
       " ['伊', '余', '心', '更', '苦'],\n",
       " ['魂', '游', '谢', '客', '诗'],\n",
       " ['朝', '庆', '千', '龄', '始'],\n",
       " ['羽', '卫', '洛', '阳', '空'],\n",
       " ['小', '径', '才', '分', '草'],\n",
       " ['当', '念', '居', '者', '思'],\n",
       " ['离', '人', '晓', '思', '惊'],\n",
       " ['堕', '枝', '伤', '翠', '羽'],\n",
       " ['莫', '上', '最', '高', '层'],\n",
       " ['鼋', '盐', '穴', '深', '水'],\n",
       " ['昔', '闻', '王', '氏', '子'],\n",
       " ['言', '与', '行', '兼', '危'],\n",
       " ['好', '是', '吴', '中', '隐'],\n",
       " ['岩', '廊', '人', '望', '在'],\n",
       " ['薄', '俸', '还', '自', '急'],\n",
       " ['含', '贞', '本', '去', '华'],\n",
       " ['尝', '登', '王', '粲', '楼'],\n",
       " ['心', '肠', '无', '邪', '欺']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator.generate(2, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
