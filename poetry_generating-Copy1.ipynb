{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry = []\n",
    "tf_word = {}\n",
    "with open('poetry.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = re.sub('（\\S+）', '', line)\n",
    "        for word in line:\n",
    "            if word not in tf_word:\n",
    "                tf_word[word] = 1\n",
    "            else:\n",
    "                tf_word[word] += 1\n",
    "        if len(line) > 15:\n",
    "            poetry += re.split('[，。！？；]', line.strip())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集诗句总数:459718, 诗句中出现过的字数:7551\n"
     ]
    }
   ],
   "source": [
    "print('训练集诗句总数:{}, 诗句中出现过的字数:{}'.format(len(poetry), len(tf_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 过滤出现次数小于该值的字\n",
    "min_tf_word = 60\n",
    "# 把每句诗长度补齐至该值\n",
    "padding_len = 5\n",
    "x_train = []\n",
    "word2idx = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in poetry:\n",
    "    words = [word if tf_word[word] > min_tf_word else '<UNK>' for word in line]\n",
    "    if len(words) > padding_len:\n",
    "        words = words[:padding_len]\n",
    "    if len(words) < padding_len:\n",
    "        words += ['<PAD>' for i in range(padding_len - len(words))]\n",
    "    x_train.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['寒', '随', '穷', '律', '变'],\n",
       " ['春', '逐', '鸟', '声', '开'],\n",
       " ['初', '风', '飘', '带', '柳'],\n",
       " ['晚', '雪', '间', '花', '梅'],\n",
       " ['碧', '林', '青', '旧', '竹'],\n",
       " ['绿', '沼', '翠', '新', '苔'],\n",
       " ['芝', '田', '初', '雁', '去'],\n",
       " ['绮', '树', '巧', '莺', '来'],\n",
       " ['晚', '霞', '聊', '自', '怡'],\n",
       " ['初', '晴', '弥', '可', '喜']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(data, path):\n",
    "    try:\n",
    "        model = Word2Vec.load(path)\n",
    "    except FileNotFoundError:\n",
    "        model = word2vec.Word2Vec(data, size=250, window=5, min_count=10, workers=12, iter=10, sg=1)\n",
    "        # size: dimensionality of word vectors\n",
    "        # min_count: ignores all words with total frequency lower than this\n",
    "        # workers: num of threads\n",
    "        # sg: 1 for skip-gram; otherwise CBOW\n",
    "        model.save(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_word2vec(x_train, 'word2vec_250.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, model):\n",
    "        self.wv = model.wv\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "        self.embedding_matrix = []\n",
    "            \n",
    "    def mk_embedding(self):\n",
    "        for i, word in enumerate(self.wv.vocab):\n",
    "            self.word2idx[word] = i\n",
    "            self.idx2word.append(word)\n",
    "            self.embedding_matrix.append(self.wv[word])\n",
    "        \n",
    "    def add_word2embedding(self, word, vector):\n",
    "        total_num = len(self.idx2word)\n",
    "        self.word2idx[word] = total_num\n",
    "        self.idx2word.append(word)\n",
    "        self.embedding_matrix.append(vector)\n",
    "        \n",
    "    def word_seq2idx_seq(self, sequences):\n",
    "        idx_seq = []\n",
    "        for sequence in sequences:\n",
    "            idx_seq.append([self.word2idx[word] for word in sequence])\n",
    "        return idx_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24], [25, 26, 27, 28, 29], [30, 31, 10, 32, 33], [34, 35, 36, 37, 38], [15, 39, 40, 41, 42], [10, 43, 44, 45, 46], [47, 48, 49, 18, 50], [11, 51, 52, 21, 27], [53, 54, 55, 56, 57], [58, 7, 8, 59, 60], [61, 62, 63, 64, 65], [66, 67, 68, 69, 70], [71, 72, 5, 73, 74], [75, 76, 7, 18, 77], [78, 79, 80, 81, 82], [83, 84, 85, 86, 87]]\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding(model)\n",
    "embedding.mk_embedding()\n",
    "idx_seq = embedding.word_seq2idx_seq(x_train)\n",
    "embedding_matrix = torch.tensor(embedding.embedding_matrix, dtype=torch.float32)\n",
    "print(idx_seq[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, input_size, hidden_size, num_layers=1, requires_grad=False):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding_matrix)\n",
    "        self.embedding.weight.requires_grad = requires_grad\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        y, _ = self.lstm(x.to(dtype=torch.float32), None)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i,:], self.y[i,:]\n",
    "\n",
    "def x2y(x):\n",
    "    return x[1:] + [embedding.word2idx['<PAD>']]\n",
    "    \n",
    "x_train = np.array(idx_seq)\n",
    "y_train = np.array(list(map(lambda x: (x2y(x)), idx_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[   0    1    2    3    4]\n",
      " [   5    6    7    8    9]\n",
      " [  10   11   12   13   14]\n",
      " ...\n",
      " [ 470  674  218   18  148]\n",
      " [  18  230  114 1137 1725]\n",
      " [1249  436  452  453  470]]\n",
      "y:\n",
      "[[   1    2    3    4 1725]\n",
      " [   6    7    8    9 1725]\n",
      " [  11   12   13   14 1725]\n",
      " ...\n",
      " [ 674  218   18  148 1725]\n",
      " [ 230  114 1137 1725 1725]\n",
      " [ 436  452  453  470 1725]]\n"
     ]
    }
   ],
   "source": [
    "print('x:\\n{}\\ny:\\n{}'.format(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(PoetryDataset(x_train, y_train), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total parameters:1736500\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "model = RNN(embedding_matrix, 250, 250, 2)\n",
    "model.to(device='cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "calc_loss = nn.MSELoss()\n",
    "print('total parameters:{}'.format(sum(p.numel() for p in model.parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(pred, y):\n",
    "    count = 0.0\n",
    "    correct_count = 0.0\n",
    "    for pred_batch, true_batch in zip(pred, y):\n",
    "        for pred_vec, true_vec in zip(pred_batch, true_batch):\n",
    "            pred_word = embedding.wv.similar_by_vector(pred_vec.detach().cpu().numpy())[0][0]\n",
    "            true_word = embedding.wv.similar_by_vector(true_vec.detach().cpu().numpy())[0][0]\n",
    "            count += 1\n",
    "            if pred_word == true_word:\n",
    "                correct_count += 1\n",
    "    return correct_count / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[01/10] time:16.15(sec) MSE loss:0.03733\n",
      "epoch:[02/10] time:19.38(sec) MSE loss:0.03674\n",
      "epoch:[03/10] time:19.29(sec) MSE loss:0.03655\n",
      "epoch:[04/10] time:19.26(sec) MSE loss:0.03644\n",
      "epoch:[05/10] time:19.28(sec) MSE loss:0.03637\n",
      "epoch:[06/10] time:19.27(sec) MSE loss:0.03631\n",
      "epoch:[07/10] time:16.96(sec) MSE loss:0.03627\n",
      "epoch:[08/10] time:18.73(sec) MSE loss:0.03624\n",
      "epoch:[09/10] time:19.32(sec) MSE loss:0.03621\n",
      "epoch:[10/10] time:19.42(sec) MSE loss:0.03618\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    \n",
    "    for data in train_set:\n",
    "        x = data[0].to(device='cuda', dtype=torch.long)\n",
    "        y = model.embedding(data[1].to(device='cuda', dtype=torch.long)).squeeze()\n",
    "        \n",
    "        pred = model(x).squeeze()\n",
    "        batch_loss = calc_loss(pred, y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += batch_loss / len(train_set)\n",
    "        \n",
    "    print('epoch:[{:02d}/{:02d}] time:{:2.2f}(sec) MSE loss:{:2.5f}'.format(epoch+1, num_epoch, time.time() - start_time, train_loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model, embedding, data, num_candidate_word=3):\n",
    "        self.model = model.eval()\n",
    "        self.embedding = embedding\n",
    "        self.data = data\n",
    "        self.num_candidate_word = num_candidate_word\n",
    "        \n",
    "    def vec2word(self, vec):\n",
    "        def remove_token(word):\n",
    "            if word[0] != '<PAD>' and word[0] != '<UNK>':\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        possile_words = self.embedding.wv.similar_by_vector(vec.detach().cpu().numpy())\n",
    "        rand_idx = random.randint(0,self.num_candidate_word)\n",
    "        return list(filter(remove_token, possile_words))[rand_idx][0]\n",
    "        \n",
    "    def sample(self):\n",
    "        i = random.randint(0, self.data.shape[0]-1)\n",
    "        origin_poetry = list(map(lambda x: self.embedding.idx2word[x], self.data[i, :]))\n",
    "        return self.data[i,:], origin_poetry\n",
    "    \n",
    "    def generate(self):\n",
    "        input, original_poetry = self.sample()\n",
    "        input = torch.tensor(input, device='cuda').unsqueeze(0)\n",
    "        pred = self.model(input)\n",
    "        words = []\n",
    "        for i in range(pred.shape[1]):\n",
    "            words.append(self.vec2word(pred[0,i,:]))\n",
    "        return [original_poetry[0]] + words[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_unk(sentence):\n",
    "    unk_id = embedding.word2idx['<UNK>']\n",
    "    if unk_id in sentence:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "x_generate = np.array(list(filter(no_unk, x_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(model, embedding, x_generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['狐', '狗', '佞', '无', '粘']\n",
      "['晚', '朦', '蒹', '箔', '溶']\n",
      "['性', '幻', '无', '禋', '鸠']\n",
      "['百', '龄', '醺', '知', '曷']\n",
      "['留', '不', '蕖', '山', '潸']\n",
      "['曾', '向', '婿', '浪', '上']\n",
      "['楚', '水', '春', '望', '无']\n",
      "['振', '翎', '蕤', '潸', '匆']\n",
      "['三', '十', '朦', '家', '否']\n",
      "['不', '是', '潸', '享', '享']\n",
      "['仰', '看', '嚬', '君', '情']\n",
      "['传', '闻', '录', '录', '乙']\n",
      "['帝', '乡', '不', '彰', '台']\n",
      "['牡', '砂', '猩', '绽', '粘']\n",
      "['潘', '浙', '侬', '我', '人']\n",
      "['客', '路', '无', '淼', '淼']\n",
      "['排', '箔', '栊', '箔', '风']\n",
      "['饮', '盏', '醺', '耐', '穰']\n",
      "['上', '有', '鹂', '门', '无']\n",
      "['江', '濆', '箔', '柳', '柳']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(generator.generate())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
