{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poetry = []\n",
    "tf_word = {}\n",
    "with open('poetry.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = re.sub('（\\S+）', '', line)\n",
    "        for word in line:\n",
    "            if word not in tf_word:\n",
    "                tf_word[word] = 1\n",
    "            else:\n",
    "                tf_word[word] += 1\n",
    "        if len(line) > 15:\n",
    "            poetry += re.split('[，。！？；]', line.strip())[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_words_poetry = list(filter(lambda x: len(x) == 5, poetry))\n",
    "seven_words_poetry = list(filter(lambda x: len(x) == 7, poetry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['寒随穷律变', '春逐鸟声开', '初风飘带柳', '晚雪间花梅', '碧林青旧竹']\n",
      "['暧暧去尘昏灞岸', '飞飞轻盖指河梁', '云峰衣结千重叶', '雪岫花开几树妆', '深悲黄鹤孤舟远']\n"
     ]
    }
   ],
   "source": [
    "print(five_words_poetry[:5], seven_words_poetry[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "五言诗诗句总数:296255，七言诗诗句总数:141968\n"
     ]
    }
   ],
   "source": [
    "print('五言诗诗句总数:{}，七言诗诗句总数:{}'.format(len(five_words_poetry), len(seven_words_poetry)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_tf_word = 50\n",
    "# 过滤出现次数小于该值的字\n",
    "word_seq = []\n",
    "word2idx = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in five_words_poetry:\n",
    "    words = [word if tf_word[word] > min_tf_word else '<UNK>' for word in line]\n",
    "    word_seq.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['寒', '随', '穷', '律', '变'], ['春', '逐', '鸟', '声', '开'], ['初', '风', '飘', '带', '柳'], ['晚', '雪', '间', '花', '梅'], ['碧', '林', '青', '旧', '竹'], ['绿', '沼', '翠', '新', '苔'], ['芝', '田', '初', '雁', '去'], ['绮', '树', '巧', '莺', '来'], ['晚', '霞', '聊', '自', '怡'], ['初', '晴', '弥', '可', '喜'], ['日', '<UNK>', '百', '花', '色'], ['风', '动', '千', '林', '翠'], ['池', '鱼', '跃', '不', '同'], ['园', '鸟', '声', '还', '异'], ['寄', '言', '博', '通', '者'], ['知', '予', '物', '外', '志'], ['一', '朝', '春', '夏', '改'], ['隔', '夜', '鸟', '花', '迁'], ['阴', '阳', '深', '浅', '叶'], ['晓', '夕', '重', '轻', '烟']]\n"
     ]
    }
   ],
   "source": [
    "print(word_seq[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.idx2word = []\n",
    "        self.word2idx = {}\n",
    "    \n",
    "    def mk_embedding(self):\n",
    "        for line in self.data:\n",
    "            for word in line:\n",
    "                if word not in self.word2idx:\n",
    "                    self.word2idx[word] = len(self.idx2word)\n",
    "                    self.idx2word.append(word)\n",
    "                    \n",
    "    def word_seq2idx_seq(self, word_seq):\n",
    "        idx_seq = []\n",
    "        for line in word_seq:\n",
    "            idx_seq.append(list(map(lambda word: embedding.word2idx[word], line)))\n",
    "        return idx_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(word_seq)\n",
    "embedding.mk_embedding()\n",
    "idx_seq = embedding.word_seq2idx_seq(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13, 14], [15, 16, 17, 18, 19], [20, 21, 22, 23, 24]]\n"
     ]
    }
   ],
   "source": [
    "print(idx_seq[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PoetryDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i,:], self.y[i,:]\n",
    "\n",
    "def x2y(x):\n",
    "    return x[1:]\n",
    "    \n",
    "x_train = np.array(idx_seq)\n",
    "y_train = np.array(list(map(lambda x: (x2y(x)), idx_seq)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[   0    1    2    3    4]\n",
      " [   5    6    7    8    9]\n",
      " [  10   11   12   13   14]\n",
      " ...\n",
      " [ 173   27 1454  151  269]\n",
      " [1328   48  799 2132   48]\n",
      " [ 175 1108 1815  470 2985]]\n",
      "y:\n",
      "[[   1    2    3    4]\n",
      " [   6    7    8    9]\n",
      " [  11   12   13   14]\n",
      " ...\n",
      " [  27 1454  151  269]\n",
      " [  48  799 2132   48]\n",
      " [1108 1815  470 2985]]\n"
     ]
    }
   ],
   "source": [
    "print('x:\\n{}\\ny:\\n{}'.format(x_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = DataLoader(PoetryDataset(x_train, y_train), batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embedding = torch.nn.Embedding(*embedding_size)\n",
    "        self.embedding.weight = torch.nn.Parameter(torch.randn(*embedding_size))\n",
    "        self.embedding.weight.requires_grad = True\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(embedding_size[1], hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.classifier = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
    "                                              torch.nn.Linear(self.hidden_size, self.hidden_size//2),\n",
    "                                              torch.nn.Sigmoid(),\n",
    "                                              \n",
    "                                              torch.nn.Dropout(0.2),\n",
    "                                              torch.nn.Linear(self.hidden_size//2, self.hidden_size//4),\n",
    "                                              torch.nn.Sigmoid(),\n",
    "                                              \n",
    "                                              torch.nn.Dropout(0.2),\n",
    "                                              torch.nn.Linear(self.hidden_size//4, self.embedding_size[0]),\n",
    "                                             )\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        hidden, _ = self.lstm(x.to(dtype=torch.float32), None)\n",
    "        y = self.classifier(hidden)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(embedding.idx2word)\n",
    "# 诗句中出现的词总数\n",
    "num_word_size = 200\n",
    "# embedding后每个单词的维度\n",
    "num_hidden_size = 1024\n",
    "# LSTM隐藏层输出维度\n",
    "num_epoch = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN((num_words, num_word_size), num_word_size)\n",
    "model.to(device='cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "calc_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_acc(pred, y):\n",
    "    pred = np.argmax(pred.detach().cpu().numpy(), axis=1)\n",
    "    return (y.detach().cpu().numpy() == pred).sum() / pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001/020] time:14.95(sec) loss:6.9928 acc:0.0913\n",
      "[002/020] time:13.93(sec) loss:6.7298 acc:0.1174\n",
      "[003/020] time:14.07(sec) loss:6.5744 acc:0.1384\n",
      "[004/020] time:14.02(sec) loss:6.4646 acc:0.1575\n",
      "[005/020] time:14.11(sec) loss:6.3874 acc:0.1722\n",
      "[006/020] time:13.88(sec) loss:6.3248 acc:0.1859\n",
      "[007/020] time:14.06(sec) loss:6.2709 acc:0.1975\n",
      "[008/020] time:14.05(sec) loss:6.2246 acc:0.2095\n",
      "[009/020] time:14.06(sec) loss:6.1815 acc:0.2184\n",
      "[010/020] time:14.04(sec) loss:6.1414 acc:0.2268\n",
      "[011/020] time:13.98(sec) loss:6.1045 acc:0.2358\n",
      "[012/020] time:13.65(sec) loss:6.0707 acc:0.2432\n",
      "[013/020] time:13.71(sec) loss:6.0399 acc:0.2501\n",
      "[014/020] time:13.72(sec) loss:6.0131 acc:0.2567\n",
      "[015/020] time:13.50(sec) loss:5.9892 acc:0.2624\n",
      "[016/020] time:13.64(sec) loss:5.9663 acc:0.2663\n",
      "[017/020] time:13.65(sec) loss:5.9462 acc:0.2722\n",
      "[018/020] time:13.42(sec) loss:5.9279 acc:0.2749\n",
      "[019/020] time:13.57(sec) loss:5.9101 acc:0.2793\n",
      "[020/020] time:13.52(sec) loss:5.8932 acc:0.2854\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = torch.load('20_epoch_model.pkl')\n",
    "except FileNotFoundError:\n",
    "    model.train()\n",
    "    for epoch in range(num_epoch):\n",
    "        start_time = time.time()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "\n",
    "        for data in train_set:\n",
    "            x = data[0].to(device='cuda', dtype=torch.long)\n",
    "            y = data[1].to(device='cuda', dtype=torch.long)\n",
    "            pred = model(x)[:,:-1,:]\n",
    "            optimizer.zero_grad()\n",
    "            # 去掉预测序列的最后一个值\n",
    "            pred = pred.transpose(1,2)\n",
    "            batch_loss = calc_loss(pred, y)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += batch_loss / len(train_set)\n",
    "            train_acc += calc_acc(pred, y) / len(train_set)\n",
    "\n",
    "        print('[{:03d}/{:03d}] time:{:.2f}(sec) loss:{:.4f} acc:{:.4f}'.format(epoch+1, num_epoch, time.time()-start_time, train_loss, train_acc))\n",
    "\n",
    "    torch.save(model, '{}_epoch_model.pkl'.format(num_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, data, model, embedding):\n",
    "        self.data = data\n",
    "        self.model = model.eval()\n",
    "        self.embedding = embedding\n",
    "    \n",
    "    def idx2word(self, idx_seq):\n",
    "        return list(map(lambda x:self.embedding.idx2word[x], idx_seq[0]))\n",
    "        \n",
    "    def generate(self):\n",
    "        i = random.randint(0, len(self.data))\n",
    "        x = torch.tensor(self.data[i], device=torch.device('cuda'), dtype=torch.long).unsqueeze(0)\n",
    "        pred = self.model(x)\n",
    "        prob_pred = np.argsort(pred.detach().cpu().numpy(), axis=2)\n",
    "        idx_pred = np.zeros((1,5))\n",
    "        for i in range(5):\n",
    "            rand_word_idx = np.random.randint(0,5,1)\n",
    "            idx_pred[0,i] = prob_pred[:,i,rand_word_idx]\n",
    "        idx_pred = idx_pred.astype(np.long)\n",
    "        return [self.idx2word(x)[0]] + self.idx2word(idx_pred)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(x_train, model, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['美', '瑁', '蜍', '蜍', '玕']\n",
      "['偶', '蜍', '瑁', '蟀', '熳']\n",
      "['不', '蓉', '蜍', '玕', '仑']\n",
      "['君', '骝', '蕖', '琶', '仑']\n",
      "['相', '瑁', '蜍', '琶', '熳']\n",
      "['问', '蜍', '蜍', '努', '仑']\n",
      "['丹', '熳', '骝', '蜍', '骝']\n",
      "['城', '瑁', '蜍', '_', '岖']\n",
      "['卧', '蓉', '蜍', '蜍', '仑']\n",
      "['本', '飖', '蜍', '仑', '玕']\n",
      "['适', '仑', '玕', '鹂', '鶒']\n",
      "['离', '仑', '仑', '仑', '仑']\n",
      "['爱', '鶒', '仑', '跎', '玕']\n",
      "['从', '蜍', '蜍', '雳', '雳']\n",
      "['走', '骝', '仑', '蜍', '飖']\n",
      "['追', '蓉', '_', '琶', '鶒']\n",
      "['<UNK>', '琶', '仑', '瑁', '鹂']\n",
      "['岁', '玕', '蜍', '玕', '玕']\n",
      "['新', '跎', '郸', '鶒', '瑁']\n",
      "['凿', '酊', '_', '蜍', '酊']\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(generator.generate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
